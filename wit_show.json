{"podcast_details": {"podcast_title": "The Women in Tech Show: A Technical Podcast", "episode_title": "Microsoft Build: Responsible AI (Sarah Bird)", "episode_image": "https://wintshow.files.wordpress.com/2016/06/beige.png?fit=3000%2C3000", "episode_transcript": " I'm Edadena Salinas, software engineer and host of the Women in Tech show, a podcast about what we work on, not what it feels like to be a woman in tech. For more information about the show, go to wit.fm. In this episode, I spoke to Sarah Bird, engineering lead at Microsoft. We talked about what responsible AI is and its main components and best practices. Sarah also explained what generative AI is and gave examples of metrics to evaluate these systems. At the end, we talked about existing tools that can assist in developing AI systems. This episode is part of a series of shows featuring speakers at Microsoft Build, an annual technical conference by Microsoft. Thanks to Microsoft for sponsoring the show and letting me attend the event. Sarah, welcome to the show. Hi, Edadena. It's great to be here. I am excited to be chatting with you today because this is a topic that's coming up a lot these days, especially with all the changes we've been seeing with AI. So today we're going to be chatting a lot about responsible AI. But first, I want to talk about your background because I saw that you studied computer engineering and then later on you did a PhD in computer science. What led you to study in this field? Yeah, maybe what seems like an unusual journey, although it felt very natural when I was doing it. I started actually working in chip design and computer architecture. And as I was doing that, I was fortunate enough to work at IBM and have the experience of working on the Xbox 360 processor design, which was super fun. But I found everything I was working on when I asked people why we were doing this or, you know, what did other people do? Everyone said this is the way we've always done things. And I felt like I wanted to better understand, you know, what was the state of the art and what did different people in the industry do? And so I decided to go to graduate school, specifically starting in computer architecture. My advisor was Dave Patterson, who's well known for computer architecture. But as part of that, I realized that actually, you know, what is interesting is architecture was it was equally challenging to figure out how to program it and how parallel computing was just happening at that time, how to allocate resources, for example. So my graduate work actually ended up being about how we automatically allocate resources in the operating system, basically using machine learning. And so I went from that pretty naturally into reinforcement learning. And that's maybe where the more interesting hop into responsible AI started. This was about 10 years ago. So it was kind of some of the earlier days of the most recent wave of AI. And so we had lots of customers and people were excited about using AI. And we were talking to them about using reinforcement learning or in this case, actually a simplified version called contextual bandits and saying, this is great. It can automatically try different options and find the best match. A technique called explore exploit. And as I was talking to potential customers or people who might want to try out this research, they were proposing like, hey, could this automatically interview people? Could this be used for something that just we were like, no, should not be used to do that. And so I was fortunate enough to be sitting with some now also, you know, great minds in AI ethics like Hannah Wallach in Microsoft Research in New York City. And I started bringing up that, you know, customers were asking if they could do this kind of stuff with the technology. And that's about the time that we formed the FATE research group. And in fact, the first sort of paper I wrote on this topic was actually looking at how reinforcement learning runs experiments and comparing that to the history of experimental ethics. And so that was sort of the trajectory from computer architecture into AI ethics. And then, you know, since then, along the way, I moved over to lead the area in product because I wanted to really focus on solving these problems for real at scale in our products. And so it's been, you know, an exciting and interesting journey. And, you know, along the way, the fields changed a lot. And, of course, AI has changed a lot. So every day feels a little bit new and different. But I've been doing this for a while now. Yes. And as you're saying, it was a very natural transition. And what you're describing is software and the way it was being applied to the domain you were focused in, which was chips, then led you into this path of artificial intelligence. And recently you've been leading the cross company team that's focused on responsible AI for GitHub, Copilot, and now also the new Bing. Yeah, actually kind of all of our responsible AI engineering at Microsoft. So we innovate across three dimensions to solve response by problems. Of course, research. What is the cutting edge? What is state of the art policy, which is our office of responsible AI and our engineering systems, which represent how do we actually build these responsible solutions? How do we make them practical? How do we make them scale so they work for every application? So I lead that third part. And so as part of that, I develop our responsible tools and technologies and the platform that powers actually many of the different generative AI applications at Microsoft. And then, of course, work with those teams like GitHub, Copilot and Bing chat to ensure that we're figuring out how to do it responsibly for the complete application. When we say responsible AI, what do we mean by responsible? That's a great question. And there's certainly like philosophical debates that people can use about which terminology you want. At Microsoft, the way we think about it is we started with our AI principles and we've had those for quite a while now. And then we've had those principles that are in their fairness and inclusiveness, privacy and security, safety and reliability, and then underlying those transparency. And then, of course, under everything, accountability. And so when we're both like governing and developing, deploying and using AI systems, we want to ensure that we're meeting those principles. And so that's sort of definition, in essence, of what we're trying to achieve. Now, of course, you could then immediately ask the question of like, well, what does it mean to make a system fair? Or what does it mean to ensure that a system is reliable? And so we take those principles and break them down into goals and then requirements. And this is all represented in our responsible AI standard, which is public now. So people can go and see what we're doing. And the standard represents our definition of what does it mean to build these systems and use these systems in a way that's responsible. So at this point, the answer to that question is a very long document that programmatically sort of outlines what does it mean to do this responsibly. Yes. And I'll definitely point out to that document. And of course, it's very broad. There's a lot of pieces that you're mentioning, and there's only so much we can talk about in 30 minutes. But yes, I would really like to touch on some of these areas. So one of the first ones would be it can help understand all the motivation behind this and behind these principles that you listed like fairness, privacy, security, transparency, perhaps by talking about some examples that have come up in terms of risks in artificial intelligence. If you can just talk about a couple of examples, that would be great. Yeah. So I think that obviously now responsibly has been around for a little while. Of course, with each new type of technology, the concerns or the sort of specific examples can change. So what we're seeing right now in generative AI is pretty different than where things started. But so maybe I'll give you a couple actually fairness related examples, because it also sort of illustrates how we go from the idea of fairness to something actually in practice at Microsoft. And so fairness, when we look at it, if we talk about the fairness of an AI system, so the fairness of its behavior, which is different than perhaps, you know, like implications in terms of how it's used or something, right. If it's used for one population versus another things. But the three types of fairness that we see in a system are the first one, quality of service fairness, which is the fairness of the accuracy of the system. And this is one of the first types of examples we started seeing sort of in the community. And then, of course, in the mainstream press and everything, as represented by the Gender Shades study, which was done out of MIT in 2018. And in that work, they were showing that early facial recognition systems did not work as well for dark skinned women as light skinned men. And that's an example of this quality of service fairness. And it's super important because we don't want our systems, which are at this point, you know, it's pretty hard to do most jobs or even just like unlock your phone right without an AI system these days. And so if they're not working as well for, you know, one group of people, that's going to have long term, you know, sort of ramification. So it's really important that we're getting that quality of service fairness. The next type of fairness is what we call harms of allocation or allocational fairness. And this is where the AI system is, you know, part of allocating resources or opportunities to people. So, for example, ranking resumes or deciding who's eligible for credit. And in that case, it's not exactly the same as accuracy because of course you want the system to be accurate, but you also want to ensure that the decisions it's making have, you know, a fair distribution and things, right? You don't want to only be offering credit to one group of people and not another group of people, for example. And so for both of these, you can build out testing programs, you build data sets that represent that diversity that represent different groups of people. And then, you know, test the behavior of your system, test the accuracy or test the decisions it's making. The third type of fairness that we've seen is called representational fairness. And this is about how people are represented in AI systems. And this is the one that's super relevant right now in the generative AI time because you have an AI system that's producing stereotyping or demeaning comments about a particular group of people, right? Then that's going to have the risk of sort of perpetuating those stereotypes. But it doesn't just apply with generative AI. We saw this earlier with an example from Google where the image tagging system had labeled a group of two black adults that gorillas, right? And so that would be an example of demeaning output from the system or a demeaning representation of people. And another example was actually early on in search engines when you searched for CEO, you saw actually only men coming up. In fact, I think the first CEO was CEO Barbie, which is very newly relevant again this summer. And so that's an example of under representing or erasing a certain group. Because now, you know, if I was doing that search thinking about career possibilities, I might think that like women can't be CEOs or there aren't any women CEOs. And so there's been many of these different types of examples, you know, over the years that have helped us, you know, just solidify, I think, the importance of these systems are part of our everyday life. And so we need to make sure that they uphold the values that we care about. And certainly they aren't perpetuating existing harms or exacerbating them. Right. And so I think that's kind of why it at least reminds me every day what's at stake here. Earlier, you were mentioning some of these examples, we've seen them before in different approaches of how artificial intelligence was being implemented. And then you're talking now about generative AI and how some of these issues also come up here. For those that aren't very familiar with generative AI, can you at a high level just explain how it's different or what it consists of? Yeah, generative AI is there's kind of two things that have happened recently in AI that are interesting and they're very related to each other. One is this rise of foundation models. So it used to be that it was actually the most common case where it worked very well to just train a different model for each task. So you'd have data science teams within sort of each application or each organization building custom models for all of the different machine learning things they wanted to accomplish. And that's great because it's tailored to the application. But, you know, these models are in essence kind of small and limited. And what we've seen in the last couple of years is that actually another approach that can be worked very well, particularly for generative AI, which I'll talk about in a second, is instead training one extremely large model, foundation model, that you are exposing to many different concepts, large, large amounts of data and using that for many, many different applications. And so, for example, if we take the GPT models by OpenAI, what they are doing is training a foundation language model on large amounts of data from the Internet, for example. And the value of this is that then the model can be much more powerful and flexible and general purpose, because in this case it understands many different concepts. It understands a natural language, it understands different languages. And so you have this powerful base model to build different applications on top of. And so that's been a transition the last couple of years. But these powerful models, foundation models, have really unlocked what we're able to do. And one of the things they've unlocked is really these generative AI applications, because in addition to just understanding content, which is what we've been using AI for a lot before, you know, classifying different types of content, they can actually also generate content. And so the GPT models are designed based on a transformer model. And the idea of a transformer model is to predict the next word in a sequence. So if I start a particular sentence, then the model is going to guess how to finish it. And so it's going to keep predicting the next word and sounds really simple and basic. But it turns out that's a really incredibly powerful mechanism that results in these models being able to do a lot of different tasks. So they can generate natural language, they can chat, they can summarize. And of course, we have the same thing kind of happening now with images. And, you know, at least I'm personally thrilled to, as a person that works in AI, to be in such an exciting time where suddenly kind of last summer, particularly with GPT-4 coming out, we saw them cross the quality threshold where they're no longer just like an interesting idea and research or kind of a toy, but something that we could build major applications like Bing Chat on and just do things we could never do before. And so we're really seeing this transformative moment where the foundation models have gotten good enough that we can unlock a whole bunch of generative AI applications in the world and just use AI for things that it never did before. So that's why there's so much excitement about it now. But of course, so much responsibility required too. It is a very powerful technology. And the image example that you mentioned earlier, I think it's still a good one to understand this difference, because earlier you were talking more about AI that understands. And for the search engine example, you're looking up an image and then it's labeled differently. But in this case, now we have models that are also generating content and to tie it up with the other example where I can imagine someone just saying, oh, generate me an image of a CEO. And then that's where the issue you were talking about earlier comes up, right? Because is it just generating something that looks like a white man, right? Or is it where this can come into play, right? Yeah, that's exactly right. And in fact, frankly, that's one of the challenges with the generative image models today, is that it's very hard to go from a simple phrase like CEO and understand what the user would be looking for. And so one of the things we have to do is obviously ensure that the model can produce many different types of people for a query like CEO, but also innovating like what interfaces should look like so that the model is not just guessing what a CEO should look like or what the user wants, but instead the user is actually using this technology to get the outcome they want, to get the visual representation of CEO that they're looking for specifically. And that's actually really important to our responsible AI approach at Microsoft. We call almost all of the applications we've been shipping in this space co-pilots. And the reason for that is very intentional. We recognize that it's exciting technology, but what really makes it exciting is how people can put it to use to be more productive, to be more creative, to do things they could never do before. And so we're very much looking at designing all of these things to be tools that are for humans to get the outcome they want and not for the model to just automatically do things. So that's exactly one of these cases in image generation. We're really thinking about how can we put humans more in control of that technology and enable them to use it better to get that CEO outcome that they're actually looking for. Is this where what people have been describing as prompts coming to play? Yeah, prompts are certainly one way to do that. And that's what we're doing right now. So if folks aren't familiar with or haven't had their hands in, you know, generative AI recently, it's quite a funny business actually to develop these applications because the way that they're programmed are prompts, which are just basically a natural language program. And there's actually like two levels of this. There's what we would call the system prompt or the meta prompt, which would be kind of the program that the developer is giving to the model to say this is what I want you to be doing. And then there's the user prompt. So, for example, if you take, you know, like a simplified version of Bing chat, it would be saying in the system prompt, hey, you're an assistant for the search engine and you're able to look up documents. You read the documents, you summarize the results in the document based on the user query, you're polite, you know, what kind of tone you want, whatever sort of safety instructions you want to put in there. So you have this very long program that is in fact the system prompt. And then you have the user prompt coming in and saying, hey, I'd like to understand more about responsible AI. Right. Like maybe that's the user prompt. And so a lot of the game now is how to express a natural language to get the application to do what you want. And in some ways, that's great because natural language is really powerful. It can be very expressive, but it's also in other ways challenging, right? Because things can be very underspecified. They can be imprecise. And so I think we'll also see over time people experimenting with, you know, more different interfaces besides just kind of prompts like we're seeing today that enable us to control the models just in different ways and maybe more effectively for different types of tasks. And when you're also talking about prompts, you've mentioned how safety can be also described in them. And I know reliability and safety are two things you've also been looking at. So I'm curious, what are some example metrics that people that are developing systems like this can use to measure this? Yeah. And I'm glad that you're asking about metrics because that's actually been just absolutely essential in everything that we've been doing. The system prompt, the meta prompt turns out to be extremely powerful, right? The application behaves very differently depending on what you say in the prompt, including just small changes in the order or changes in a single word. And so great, we have this really powerful tool to change the behavior. So that's great from the point of view of like responsible AI that we have that amount of control. But figuring out, you know, it's not completely obvious, like what's going to work well and what's not going to work well. And so, for example, when we were developing Bing Chat, one of the things we had to do is develop metrics for an automated measurement system for all of the different responsible AI risks we're concerned about. And so this would be measuring everything from sort of errors that the system is making, where the system is coming ungrounded, which I'll explain is how we actually think about hallucination. So hallucination is a term that you'll see externally some. And the idea about hallucination is the model is just making up information or providing incorrect information. So in the case of our applications at Microsoft, we're giving the model accurate data, like we're having it look at the search engine and respond based on the search engine. And so we want to ensure that it's staying grounded. It's only using that data we're giving it. It's not adding additional information or changing it. So we have a metric to measure that the model staying grounded in the data. We have metrics to measure whether or not it's producing harmful content or if it's producing harmful code or, for example, has an inappropriate tone or it's behaving in a way that's too human-like and is inappropriate. And so across these different dimensions, we develop automated measurements and that enables us to, you know, any time we're making a change or testing different meta prompts or testing other changes in the application, meta prompts aren't the only part. Then we're able to run it against all of those tests, all of those metrics and understand if we're, you know, we're making an improvement or we're opening up some additional risk. And so the measurement part of the story has been really, really key. And, you know, the result of that is we found we do it does work well to have a fairly large section for response where you basically have meta prompt instructions that are sort of tailored to each of these different types of risks like the model and not staying grounded. Or, for example, the model being deceptible to adversarial attacks like jail breaks. Right. And so we end up having fairly a lot of different responsibly things in the meta prompt. At the beginning of this show, you're talking about these three dimensions of the things you and your team have been working on. The first one was research, then policy, and also engineering systems, which makes me think a lot of what you're describing just now where people that are developing this can have access to, I don't know, a list of metrics or are there already now tools that can help you automate this or plug them in as part of your flow. For developing systems using generative AI. Yeah. And, you know, that's exactly kind of how we've been able to do this at Microsoft and develop so many different, you know, co-pilots in the last six months because to do like the first application, you know, it takes a lot of our AI experts, right? We had the full experts from around the company to developing chat. But as part of that, and, you know, Bing chat wasn't the first generative application we've shipped. We've been working for this for quite a few years we had previously done, you know, get a co-pilot. And, you know, with each of these we learn sort of what's needed. We of course develop what's needed for that particular application, but we always develop in a way where we're actually bringing that to our AI platform and so in Azure AI, we take the technologies we've developed, make them part of our AI platform, which enables all of the other teams in Microsoft, the next application we build to leverage the technology we already built, whether that's mitigation technologies like our safety system that, you know, monitors in real time and locks, you know, harmful content or different attacks, or whether that's the measurement system and the built in metrics. And so we have that in our AI platform and all of our internal applications are built on that. And, you know, this year at Build we started announcing some of these things available for our customers who are also building generative AI applications. And so we announced Azure AI Content Safety, which is our safety system that we build into our applications, as well as some metrics that people can use as part of Azure machine learning. And so you can go and develop your generative AI application in Promflow and you can measure for things like groundedness that I was just talking about or coherence or relevance. And so we also have like prompt engineering guides and red teaming, which is a practice for manually stress testing your system. And so as much as possible. Yeah, we're trying to turn everything either into just technology built into the platform or tools that you can use to develop your own application, or of course, you know, guidance and best practices based on what we're learning. And so that's enabling us to take, you know, the amount of expertise we have and really scale it out so that it's benefiting, you know, every application inside of Microsoft and hopefully many applications outside of Microsoft as well. As you mentioned, these tools and technologies were announced at Microsoft Build, which brings me to my last question, because the reason I invited you on the show was because I came across the talk that you were going to give. So my last question is, what did you like about Microsoft Build conference? Well, first of all, this year was the first year it was in person in a little while and it's just exciting. There's so much energy having that many people in the room and in my case that many people in the room excited about AI. I think the other thing that I really liked about it is actually where you started this talk of, hey, you know, response to AI is a pretty hot topic. And well, response to AI is certainly a topic, a hot topic now and everybody is, you know, very interested in hearing about the tools and technology that we've developed or the work we're doing. It's something that I've been working on and many others for years, right? We started this research group and like our early work in this space. I think, you know, starting to be close to, you know, almost a decade ago. And so it's just so exciting to be at a conference like Build and see how much has changed, like see how much the rest of the world is engaged in this now. See how much the work has matured, right? Every time I ship a product, I'm thinking, wow, we've come so far. I remember when it was like just the early days and we had, you know, a few people trying to figure it out to now we're shipping like full products like Azure Content Safety for responsible AI. And so I think for me it was a really special moment in terms of just, you know, recognizing where we are and how far we've come. And of course, you know, very humbling how far we still have to go, right? It's definitely the early days. But I think Build, you know, was really exciting to just kind of put an emphasis on that moment. Just definitely working on the challenges for real systems and at scale, like you were mentioning earlier. So, Sarah, I want to thank you for coming on the show. I know we've been meaning to schedule this. So I really appreciate you taking the time to come on. Yeah, I'm so glad I could be here. Thank you for inviting me and for, you know, having such a great show."}, "podcast_summary": "In this episode of the Women in Tech show, host Edadina Salinas speaks with Sarah Bird, an engineering lead at Microsoft, about responsible AI. They discuss the main components and best practices of responsible AI, as well as the concept of generative AI and how it can be evaluated using metrics. Sarah also mentions the existing tools available for developing AI systems. The episode highlights the importance of fairness, privacy, security, transparency, and accountability in responsible AI, and the need to uphold these principles in order to avoid perpetuating biases or harm. Sarah emphasizes the role of metrics in measuring and ensuring the safety and reliability of AI systems. The episode concludes with a discussion on Microsoft's efforts to integrate responsible AI practices into their products and platforms.", "podcast_guest": {"name": "Sarah Bird", "org": "", "title": "", "summary": "Not Available"}, "podcast_highlights": "Highlight 1 of the podcast: \"Fairness of an AI system can be measured through different types of fairness, such as quality of service fairness, harms of allocation, and representational fairness.\" This highlights the importance of fairness in AI systems and the need to measure and address different aspects of fairness.\n\nHighlight 2 of the podcast: \"Generative AI is a powerful technology that can generate content, such as natural language and images. It is enabled by training large foundation models that understand different concepts and can be used for various applications.\" This highlights the transformative potential of generative AI and the use of foundation models to power these applications.\n\nHighlight 3 of the podcast: \"Developing responsible AI systems requires the use of metrics to measure risks, such as grounding, coherence, relevance, and safety. These metrics help ensure that the system behaves ethically and meets responsible AI standards.\" This highlights the importance of metrics in developing and evaluating responsible AI systems.\n\nHighlight 4 of the podcast: \"Microsoft has developed tools and technologies, such as Azure AI Content Safety and Azure machine learning metrics, to support the development of generative AI applications and ensure their responsible use. These tools and technologies are available for both internal and external developers.\" This highlights Microsoft's commitment to responsible AI and the availability of resources to help developers create responsible AI applications.\n\nHighlight 5 of the podcast: \"The Microsoft Build conference provided an opportunity to showcase the progress in responsible AI and the maturity of the field. It highlighted the impact and potential of responsible AI and the continuous work that still needs to be done.\" This highlights the significance of the Microsoft Build conference and the recognition of the progress made in responsible AI, as well as the ongoing challenges in the field."}